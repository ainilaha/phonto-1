---
layout: default
title: "Diagnostics: Codebook Inconsistencies"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
library(knitr)
opts_chunk$set(collapse = FALSE, autodep = TRUE, 
               comment = "", warning = TRUE, message = TRUE, prompt = FALSE,
               fig.path = "figures/diagnostics-codebook-",
               out.width = "100%",
               fig.width = 12, fig.height = 8,
               dev = "svglite", dev.args = list(pointsize = 12),
               cache = TRUE,
               cache.path = "~/knitr-cache/diagnostics-codebook/")
options(warnPartialMatchDollar = FALSE, width = 85)
```



NHANES tables themselves have cryptic variable names, and must be used
in conjunction with corresponding documentation files to be
interpreted. Both standard and database versions of the `nhanes()` and
`nhanesFromURL()` functions in the __nhanesA__ package return a
"translated" data frame, which modify the raw data columns in the SAS
transport files using per-variable translation tables, referred to as
_codebooks_, obtained from the NHANES online documentation.

This document describes a series of diagnostic checks to identify
possible issues with these codebooks.

# Variable codebooks

Variable codebooks are obtained by downloading and parsing online
documentation files. These codebooks are stored in the database,
making it relatively easy to work with them.

```{r}
library(nhanesA)
library(phonto)
all_cb <- nhanesQuery("select * from Metadata.VariableCodebook")
str(all_cb)
```


# Ambiguous variable types

NHANES has both numeric and categorical variables. There is no
indication in the data or documentation itself of what type a certain
variable is supposed to be. However, for most numeric variables, the
`ValueDescription` column will have an entry called `"Range of
Values"`. The presence of this value is used by the __nhanesA__
package to infer the type of a variable.

Unfortunately, with this rule, some variables are flagged as numeric
in some cycles but categorical in others. Such variables can be
identified in the searchable variable tables available [here](../)
with a `Type` value of `ambiguous`. Below, we try to take a closer
look at such variables.

We first restrict our attention to variables that are 'numeric' in at
least one table. There may be others that are mistakenly classified as
numeric, but those may be difficult to flag.

```{r, warning = FALSE}
numeric_vars <- with(all_cb, unique(Variable[ValueDescription == "Range of Values"]))
numeric_cb <- subset(all_cb, Variable %in% numeric_vars, select = 1:5)
```

Ideally, all the 'numeric' values in these codebooks should be
identified as `"Range of Values"`. If they are not, however, they are
usually just the numeric value, or some indicator of thresholding such
as `"more than 80"`. Let us look at the 'ValueDescription'-s that
represent numeric values, in the sense that they can be coerced to a
finite numeric value.


```{r}
maybe_numeric <- is.finite(as.numeric(numeric_cb$ValueDescription))
table(maybe_numeric)
```

We will focus on these variables for now.

```{r}
problem_vars <- unique(numeric_cb[maybe_numeric, ]$Variable)
str(problem_vars)
length(num_cb_byVar <- numeric_cb |>
           subset(Variable %in% problem_vars) |>
           split(~ Variable))
```

Let's start by summarizing these to keep only the unique
`CodeOrValue` + `ValueDescription` combinations, and then prioritize
them by the number of numeric-like values that remain.

```{r}
summary_byVar <-
    lapply(num_cb_byVar,
           function(d) unique(d[c("Variable", "CodeOrValue",
                                  "ValueDescription")]))
numNumeric <- function(d) {
    suppressWarnings(sum(is.finite(as.numeric(d$ValueDescription))))
}
(nnum <- sapply(summary_byVar, numNumeric) |> sort())
```

To get a sense of the problem cases, we look at the variables with 10
or more numeric variables.

```{r}
num_cb_byVar[ names(which(nnum >= 10)) ]
```

## What to do about these?

The last example is of particular concern, because the `KID221`
variable clearly means different things in different
tables. Otherwise, these all look like legitimate issues, and there
are not many of them, so a possible workaround is to maintain an
explicit list of such variables and handle them while creating the
codebook. The least intrusive way would be to just insert a row with
value description `"Range of Values"`, and perhaps drop the value
descriptions which can be coerced to numeric.


```{r, echo = FALSE, eval = FALSE}
## Detecting thresholding keywords

## An alternative approach to finding variables that are possibly
## numeric but not identified as such is to look for phrases that
## indicate thresholding, such as "more than" or "less than".

with(all_cb, unique(c(grep("^more than", ValueDescription, ignore.case = TRUE, value = TRUE),
                      grep("^less than", ValueDescription, ignore.case = TRUE, value = TRUE))))
threshold_vars <-
    with(all_cb,
         unique(Variable[grepl("^more than", ValueDescription, ignore.case = TRUE) |
                         grepl("^less than", ValueDescription, ignore.case = TRUE)]))
categorical_vars <- setdiff(unique(all_cb$Variable), numeric_vars)
threshold_cb <- subset(all_cb,
                       Variable %in% intersect(categorical_vars, threshold_vars),
                       select = 1:5)
table(threshold_cb$Variable)
split(threshold_cb, ~ Variable)
```


# Codebook conversion problems

Ideally, each codebook (as returned by `nhanesCodebook()` should
contain one element for each variable in the table, where each element
is a list containing information about that variable. This information
currently consists of the 'SAS Label', 'English Text', and 'Target',
as recorded in the documentation files, along with a translation table
with descriptions of the codes used in the data.

The following functions checks to see if a given codebook satisfies
these expectations. In addition to checking for the presence of a
translation table, it flags cases where a potentially numeric variable
has unusual codes, accounting for some common non-response codes and
thresholding codes.

```{r}
acceptable <-
    c("Range of Values", "Missing", "No response", "Refused", "Refuse",
      "SP refused", "Could not obtain", "No Lab Result", "No lab specimen", 
      "Don't know", "Don't  Know", "Cannot be assessed",
      "Calculation cannot be determined", "Since birth",
      "Fill Value of Limit of Detection", "Below Limit of Detection",
      "None", "Never")
agelimits <-
    c("80 years or older", "85 years or older",
      ">= 80 years of age", ">= 85 years of age", "80 years of age and over",
      "9 or younger", "9 years or younger",
      "12 years or younger ", "14 years or younger",
      "45 years or older", "14 years or under",
      "60 years or older")
var_status <- function(v, cb) {
    x <- cb[[v]][[v]]
    if (is.null(x)) return(NA) # no info, usually for SEQN
    probablyNumeric <- "Range of Values" %in% x$Value.Description
    if (!probablyNumeric) return(TRUE) # OK - at least for now
    ok <- all(tolower(x$Value.Description) %in% tolower(c(acceptable, agelimits)))
    ok
}
find_conversion_problems <- function(nh_table)
{
    cb <- nhanesCodebook(nh_table)
    cb_status <- vapply(names(cb), var_status, logical(1), cb = cb)
    if (all(is.na(cb_status))) "INVALID CODEBOOK" # the whole table is problematic ?
    else lapply(cb[ !is.na(cb_status) & !cb_status ],
                function(x) x[[length(x)]][1:3])
}
```

These are used below to find potential problems in importing codebooks.

```{r}
tables <- nhanesQuery("select TableName from Metadata.QuestionnaireDescriptions")$TableName
status <- lapply(tables, find_conversion_problems)
names(status) <- tables
keep <- sapply(status, length) > 0 # tables with some issues
status <- status[keep]
tables <- tables[keep]
```

## Tables with no useful codebook in the database

```{r}
no_codebook <- sapply(status, identical, "INVALID CODEBOOK")
cat(format(tables[no_codebook]), fill = TRUE)
```

`ALB_CR_G` is a known example where there are no translation tables;
this is not a problem because all variables are numeric and do not
require translation. Other instances should be investigated.


## Tables with unexpected value descriptions

Most of the remaining 'problems' arise from special numeric codes,
which are perhaps too many to deal with systematically, but do need to
be accounted for during analysis. They are listed below for reference.

```{r}
labels_df <- status[!no_codebook] |>
    do.call(what = c) |> do.call(what = rbind)
## keep only value and description
labels_df <- labels_df[1:2]
```

Next, we count the number of variables each description occurs in, and
sort by frequency.

```{r}
labels_df <- subset(labels_df, Value.Description != "Range of Values")
labels_split <- split(labels_df, ~ Value.Description)
labels_summary <-
    lapply(labels_split,
           function(d) with(d,
                            data.frame(Desc = substring(as.character(Value.Description)[[1]],
                                                        1, 45),
                                       Count = length(Value.Description),
                                       Codes = sort(unique(Code.or.Value))
                                                 |> paste(collapse = "/")
                                                 |> substring(1, 30)))) |>
    do.call(what = rbind)
```

```{r}
options(width = 200)
rownames(labels_summary) <- NULL
labels_summary[order(labels_summary$Count, decreasing = TRUE), ]
```

